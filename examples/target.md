  

## **Zhiyong Wu**  

![alt text](https://lividwo.github.io/zywu.github.io/photos/bio.jpeg) |

  
Research Scientist  
[Shanghai AI Laboratory](https://www.shlab.org.cn/)  

Email: a@b, a=whucs2013wzy b=gmail.com]  
[[Github](https://github.com/LividWo)] [[Google
Scholar](https://scholar.google.com/citations?user=wIlpfXEAAAAJ&hl=en)]  
  
---|---  
  
## About me

Hi! I am a research scientist at Shanghai AI Lab. I got my PhD degree from the
University of Hong Kong at the end of 2021, affiliated with the HKU database
group and [ NLP group](https://nlp.cs.hku.hk/). I am advised by Prof. [Ben
Kao](https://www.cs.hku.hk/people/academic-staff/kao). I am also working
closely with [Dr. Lingpeng Kong](https://ikekonglp.github.io/index.html).
Before that, I received my B.E. degree from the Dept. of Computer Science at
[Wuhan University](https://www.whu.edu.cn/en/) in 2017. Throughout my graduate
studies, I had great internships in Tencent AI Lab and Huawei Noah's Ark Lab.

## Hiring

We have multiple full-time/internship positions available (focus on language
agent and multilingual LLM), please feel free to hit me up with your CV or
questions if interested.

## Research

I am boardly interested in different topics in NLP. But at the moment, my
research focus on exploring interesting (sometimes surprising) utilities of
large language models:

  * To synthesis datasets without human annotation. ([ZeroGen](https://arxiv.org/abs/2202.07922), [ProGen](https://arxiv.org/abs/2211.11160), [SunGen](https://openreview.net/forum?id=h5OpjGd_lo6))

  * To explain model decision via natural language generation. ([Neon](https://arxiv.org/abs/2211.11160), [EIB](https://arxiv.org/abs/2212.09603))

  * To learn a task without training by conditioning on in-context examples. ([SAIL](https://arxiv.org/abs/2212.10375), [CEIL](https://arxiv.org/abs/2302.05698), [EvaLM](https://arxiv.org/pdf/2302.04931.pdf), [survey](https://arxiv.org/pdf/2301.00234.pdf), [OpenICL](https://arxiv.org/abs/2303.02913))

I'm currently obsessed with the idea of "LLM-powered autonomous agents" and
have multiple related projects underway. If you are also interested in this
topic and have a plan to do an internship, feel free to hit me up via email.
[Research output of my
interns](https://lividwo.github.io/zywu.github.io/#interns)

## Publications

(*: equal contribution)

**Preprints**

  1. In-Context Learning with Many Demonstration Examples  
Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang, **Zhiyong Wu** ,
Lingpeng Kong.  
[[pdf](https://arxiv.org/pdf/2302.04931.pdf)].

  2. A Survey on In-context Learning  
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, **Zhiyong Wu** , Baobao Chang, Xu
Sun, Jingjing Xu, Lei Li, Zhifang Sui  
[[pdf](https://arxiv.org/pdf/2301.00234.pdf)].

  3. Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration  
Qiushi Sun, Zhangyue Yin, Xiang Li, **Zhiyong Wu** , Xipeng Qiu, Lingpeng Kong  
[[pdf](https://arxiv.org/abs/2310.00280)].

  4. EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling  
Siyu Ren, **Zhiyong Wu** , Kenny Q Zhu  
[[pdf](https://arxiv.org/abs/2310.04691)].

**2023**

  5. Can We Edit Factual Knowledge by In-Context Learning?  
Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, **Zhiyong Wu** , Jingjing Xu,
Baobao Chang  
EMNLP 2023, Singapore, [[pdf](https://arxiv.org/abs/2305.12740)].
[[code](https://github.com/Zce1112zslx/IKE)]

  6. DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models  
Shansan Gong, Mukai Li, Jiangtao Feng, **Zhiyong Wu** , Lingpeng Kong.  
EMNLP 2023, Findings, Singapore, [[pdf](https://arxiv.org/abs/2310.05793)].
[[code](https://github.com/Shark-NLP/DiffuSeq)]

  7. Self-adaptive In-context Learning  
**Zhiyong Wu** *, Yaoxiang Wang*, Jiacheng Ye*, Lingpeng Kong.  
ACL 2023, Toronto, [[pdf](https://arxiv.org/abs/2212.10375)].
[[code](https://github.com/Shark-NLP/self-adaptive-ICL)]

  8. OpenICL: An Open-Source Framework for In-context Learning  
Zhenyu Wu*, YaoXiang Wang*, Jiacheng Ye*, Jiangtao Feng, Jingjing Xu, Yu Qiao,
**Zhiyong Wu**.  
ACL 2023, Toronto, Demo paper, [[pdf](https://arxiv.org/abs/2303.02913)].
[[code](https://github.com/Shark-NLP/OpenICL)]

  9. Explanation Regeneration via Information Bottleneck  
Qintong Li, **Zhiyong Wu** , Lingpeng Kong, Wei Bi.  
ACL 2023 Findings, Toronto, [[pdf](https://arxiv.org/abs/2212.09603)].

  10. Compositional Exemplars for In-context Learning  
Jiacheng Ye, **Zhiyong Wu** , Jiangtao Feng, Tao Yu, Lingpeng Kong.  
ICML 2023, Hawaii, [[pdf](https://arxiv.org/abs/2302.05698)].
[[code](https://github.com/HKUNLP/icl-ceil)]

  11. DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models  
Shansan Gong, Mukai Li, Jiangtao Feng, **Zhiyong Wu** , Lingpeng Kong.  
ICLR 2023, Rwanda, [[pdf](https://arxiv.org/abs/2210.08933)].
[[code](https://github.com/Shark-NLP/DiffuSeq)]

  12. Self-Guided High-Quality Data Generation in Efficient Zero-Shot Learning  
Jiahui Gao, Renjie Pi, Yong Lin, Hang Xu, Jiacheng Ye, **Zhiyong Wu** ,
Xiaodan Liang, Zhenguo Li, Lingpeng Kong.  
ICLR 2023, Rwanda, [[pdf](https://openreview.net/forum?id=h5OpjGd_lo6)].

  13. Unsupervised Explanation Generation via Correct Instantiations  
Sijie Chen, **Zhiyong Wu** , Jiangjie Chen, Zhixing Li, Yang Liu, and Lingpeng
Kong  
AAAI 2023, Washington, [[pdf](https://arxiv.org/abs/2211.11160)].
[[code](https://github.com/Shark-NLP/Neon)]

**2022**

  14. ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback  
Jiacheng Ye, Jiahui Gao, **Zhiyong Wu** , Jiangtao Feng, Tao Yu, and Lingpeng
Kong.  
EMNLP-Findings 2022, long paper.[[pdf](https://arxiv.org/abs/2210.12329)].

  15. ZeroGen: Efficient Zero-shot Learning via Dataset Generation  
Jiacheng Ye*, Jiahui Gao*, Qintong Li, Hang Xu, Jiangtao Feng, **Zhiyong Wu**
, Tao Yu and Lingpeng Kong.  
EMNLP 2022, long paper. [[pdf](https://arxiv.org/abs/2202.07922)].
[[code](https://github.com/jiacheng-ye/zerogen)]

  16. Lexical Knowledge Internalization for Neural Conversational Models  
**Zhiyong Wu** , Wei Bi, Xiang Li, Lingpeng Kong, Ben Kao.  
ACL 2022, long paper. [[pdf](https://arxiv.org/abs/2205.01941)].
[[code](https://github.com/LividWo/KI)]

  17. COLO: A Contrastive Learning based Re-ranking Framework for One-Stage Summarization  
Chenxin An, Ming Zhong, **Zhiyong Wu** , Qin Zhu, Xuanjing Huang, Xipeng Qiu.  
COLING 2022, long paper. [[pdf](https://arxiv.org/pdf/2209.14569.pdf)].
[[code](https://github.com/ChenxinAn-fdu/CoLo)]

**2021**

  18. Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation  
**Zhiyong Wu** , Lingpeng Kong, Wei Bi, Xiang Li, Ben Kao.  
ACL 2021, long paper. [[pdf](https://arxiv.org/pdf/2105.14462.pdf)]
[[code](https://github.com/LividWo/Revisit-MMT)]

  19. Cascaded Head-colliding Attention  
Lin Zheng, **Zhiyong Wu** , Lingpeng Kong.  
ACL 2021, long paper. [[pdf](https://arxiv.org/pdf/2105.14850.pdf)]
[[code](https://zywu.github.io/pub/wsdm2020.pdf)]

**2020 and before**

  20. Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT  
**Zhiyong Wu** , Yun Chen, Ben Kao, Qun Liu.  
ACL 2020. [[pdf](https://arxiv.org/abs/2004.14786)]
[[code](https://github.com/LividWo/Perturbed-Masking)]  

  21. PERQ: Predicting, Explaining, and Rectifying Failed Questions in KB-QA Systems   
**Zhiyong Wu** , Ben Kao, Tien-Hsuan Wu, Pengcheng Yin, Qun Liu.  
WSDM 2020, long paper. [[pdf](https://zywu.github.io/pub/wsdm2020.pdf)]  

  22. Towards Practical Open Knowledge Base Canonicalization   
TTien-Hsuan Wu, **Zhiyong Wu** , Ben Kao, Pengcheng Yin.  
CIKM 2018.
[[pdf](https://www.cs.hku.hk/data/techreps/document/TR-2018-04.pdf)]  

## Interns

[Jiacheng Ye](https://jiacheng-ye.github.io/) |
[EMNLP'22a](https://arxiv.org/pdf/2209.14569.pdf),
[EMNLP'22b](https://arxiv.org/abs/2210.12329),
[ICML'23](https://arxiv.org/abs/2302.05698)  
---|---  
[Sijie Cheng](https://adacheng.github.io/) |
[AAAI'23](https://arxiv.org/abs/2211.11160)  
[ Yaoxiang Wang](https://scholar.google.com/citations?user=7e_BZuYAAAAJ&hl=zh-
CN) | [ACL'23a](https://arxiv.org/abs/2212.10375),
[ACL'23b](https://arxiv.org/abs/2303.02913)  
[Zhenyu Wu](https://github.com/numbmelon) |
[ACL'23b](https://arxiv.org/abs/2303.02913)  
[Siyu Ren](https://drsy.github.io/) | [Under review at
ICLR'24](https://arxiv.org/abs/2310.04691)  
[Qiushi Sun](https://qiushisun.github.io/) | [Under review at
ICLR'24](https://arxiv.org/abs/2310.00280)  
[Fangzhi Xu](https://xufangzhi.github.io/) | TBA  
[Kanzhi Cheng](https://scholar.google.com/citations?user=S2IPVnwAAAAJ&hl=zh-
CN) | TBA  
[Yi Lu](https://scholar.google.com/citations?user=WK62eYQAAAAJ&hl=zh-CN) | TBA

