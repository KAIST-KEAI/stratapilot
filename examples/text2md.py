import re
import json


# 示例字符串
data = '''
2023-12-28 20:51:35,475 - INFO - {'markdown': '  \n\n## **Zhiyong Wu**  \n\n![alt text](https://lividwo.github.io/zywu.github.io/photos/bio.jpeg) |\n\n  \nResearch Scientist  \n[Shanghai AI Laboratory](https://www.shlab.org.cn/)  \n\nEmail: a@b, a=whucs2013wzy b=gmail.com]  \n[[Github](https://github.com/LividWo)] [[Google\nScholar](https://scholar.google.com/citations?user=wIlpfXEAAAAJ&hl=en)]  \n  \n---|---  \n  \n## About me\n\nHi! I am a research scientist at Shanghai AI Lab. I got my PhD degree from the\nUniversity of Hong Kong at the end of 2021, affiliated with the HKU database\ngroup and [ NLP group](https://nlp.cs.hku.hk/). I am advised by Prof. [Ben\nKao](https://www.cs.hku.hk/people/academic-staff/kao). I am also working\nclosely with [Dr. Lingpeng Kong](https://ikekonglp.github.io/index.html).\nBefore that, I received my B.E. degree from the Dept. of Computer Science at\n[Wuhan University](https://www.whu.edu.cn/en/) in 2017. Throughout my graduate\nstudies, I had great internships in Tencent AI Lab and Huawei Noah\'s Ark Lab.\n\n## Hiring\n\nWe have multiple full-time/internship positions available (focus on language\nagent and multilingual LLM), please feel free to hit me up with your CV or\nquestions if interested.\n\n## Research\n\nI am boardly interested in different topics in NLP. But at the moment, my\nresearch focus on exploring interesting (sometimes surprising) utilities of\nlarge language models:\n\n  * To synthesis datasets without human annotation. ([ZeroGen](https://arxiv.org/abs/2202.07922), [ProGen](https://arxiv.org/abs/2211.11160), [SunGen](https://openreview.net/forum?id=h5OpjGd_lo6))\n\n  * To explain model decision via natural language generation. ([Neon](https://arxiv.org/abs/2211.11160), [EIB](https://arxiv.org/abs/2212.09603))\n\n  * To learn a task without training by conditioning on in-context examples. ([SAIL](https://arxiv.org/abs/2212.10375), [CEIL](https://arxiv.org/abs/2302.05698), [EvaLM](https://arxiv.org/pdf/2302.04931.pdf), [survey](https://arxiv.org/pdf/2301.00234.pdf), [OpenICL](https://arxiv.org/abs/2303.02913))\n\nI\'m currently obsessed with the idea of "LLM-powered autonomous agents" and\nhave multiple related projects underway. If you are also interested in this\ntopic and have a plan to do an internship, feel free to hit me up via email.\n[Research output of my\ninterns](https://lividwo.github.io/zywu.github.io/#interns)\n\n## Publications\n\n(*: equal contribution)\n\n**Preprints**\n\n  1. In-Context Learning with Many Demonstration Examples  \nMukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang, **Zhiyong Wu** ,\nLingpeng Kong.  \n[[pdf](https://arxiv.org/pdf/2302.04931.pdf)].\n\n  2. A Survey on In-context Learning  \nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, **Zhiyong Wu** , Baobao Chang, Xu\nSun, Jingjing Xu, Lei Li, Zhifang Sui  \n[[pdf](https://arxiv.org/pdf/2301.00234.pdf)].\n\n  3. Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration  \nQiushi Sun, Zhangyue Yin, Xiang Li, **Zhiyong Wu** , Xipeng Qiu, Lingpeng Kong  \n[[pdf](https://arxiv.org/abs/2310.00280)].\n\n  4. EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling  \nSiyu Ren, **Zhiyong Wu** , Kenny Q Zhu  \n[[pdf](https://arxiv.org/abs/2310.04691)].\n\n**2023**\n\n  5. Can We Edit Factual Knowledge by In-Context Learning?  \nCe Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, **Zhiyong Wu** , Jingjing Xu,\nBaobao Chang  \nEMNLP 2023, Singapore, [[pdf](https://arxiv.org/abs/2305.12740)].\n[[code](https://github.com/Zce1112zslx/IKE)]\n\n  6. DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models  \nShansan Gong, Mukai Li, Jiangtao Feng, **Zhiyong Wu** , Lingpeng Kong.  \nEMNLP 2023, Findings, Singapore, [[pdf](https://arxiv.org/abs/2310.05793)].\n[[code](https://github.com/Shark-NLP/DiffuSeq)]\n\n  7. Self-adaptive In-context Learning  \n**Zhiyong Wu** *, Yaoxiang Wang*, Jiacheng Ye*, Lingpeng Kong.  \nACL 2023, Toronto, [[pdf](https://arxiv.org/abs/2212.10375)].\n[[code](https://github.com/Shark-NLP/self-adaptive-ICL)]\n\n  8. OpenICL: An Open-Source Framework for In-context Learning  \nZhenyu Wu*, YaoXiang Wang*, Jiacheng Ye*, Jiangtao Feng, Jingjing Xu, Yu Qiao,\n**Zhiyong Wu**.  \nACL 2023, Toronto, Demo paper, [[pdf](https://arxiv.org/abs/2303.02913)].\n[[code](https://github.com/Shark-NLP/OpenICL)]\n\n  9. Explanation Regeneration via Information Bottleneck  \nQintong Li, **Zhiyong Wu** , Lingpeng Kong, Wei Bi.  \nACL 2023 Findings, Toronto, [[pdf](https://arxiv.org/abs/2212.09603)].\n\n  10. Compositional Exemplars for In-context Learning  \nJiacheng Ye, **Zhiyong Wu** , Jiangtao Feng, Tao Yu, Lingpeng Kong.  \nICML 2023, Hawaii, [[pdf](https://arxiv.org/abs/2302.05698)].\n[[code](https://github.com/HKUNLP/icl-ceil)]\n\n  11. DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models  \nShansan Gong, Mukai Li, Jiangtao Feng, **Zhiyong Wu** , Lingpeng Kong.  \nICLR 2023, Rwanda, [[pdf](https://arxiv.org/abs/2210.08933)].\n[[code](https://github.com/Shark-NLP/DiffuSeq)]\n\n  12. Self-Guided High-Quality Data Generation in Efficient Zero-Shot Learning  \nJiahui Gao, Renjie Pi, Yong Lin, Hang Xu, Jiacheng Ye, **Zhiyong Wu** ,\nXiaodan Liang, Zhenguo Li, Lingpeng Kong.  \nICLR 2023, Rwanda, [[pdf](https://openreview.net/forum?id=h5OpjGd_lo6)].\n\n  13. Unsupervised Explanation Generation via Correct Instantiations  \nSijie Chen, **Zhiyong Wu** , Jiangjie Chen, Zhixing Li, Yang Liu, and Lingpeng\nKong  \nAAAI 2023, Washington, [[pdf](https://arxiv.org/abs/2211.11160)].\n[[code](https://github.com/Shark-NLP/Neon)]\n\n**2022**\n\n  14. ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback  \nJiacheng Ye, Jiahui Gao, **Zhiyong Wu** , Jiangtao Feng, Tao Yu, and Lingpeng\nKong.  \nEMNLP-Findings 2022, long paper.[[pdf](https://arxiv.org/abs/2210.12329)].\n\n  15. ZeroGen: Efficient Zero-shot Learning via Dataset Generation  \nJiacheng Ye*, Jiahui Gao*, Qintong Li, Hang Xu, Jiangtao Feng, **Zhiyong Wu**\n, Tao Yu and Lingpeng Kong.  \nEMNLP 2022, long paper. [[pdf](https://arxiv.org/abs/2202.07922)].\n[[code](https://github.com/jiacheng-ye/zerogen)]\n\n  16. Lexical Knowledge Internalization for Neural Conversational Models  \n**Zhiyong Wu** , Wei Bi, Xiang Li, Lingpeng Kong, Ben Kao.  \nACL 2022, long paper. [[pdf](https://arxiv.org/abs/2205.01941)].\n[[code](https://github.com/LividWo/KI)]\n\n  17. COLO: A Contrastive Learning based Re-ranking Framework for One-Stage Summarization  \nChenxin An, Ming Zhong, **Zhiyong Wu** , Qin Zhu, Xuanjing Huang, Xipeng Qiu.  \nCOLING 2022, long paper. [[pdf](https://arxiv.org/pdf/2209.14569.pdf)].\n[[code](https://github.com/ChenxinAn-fdu/CoLo)]\n\n**2021**\n\n  18. Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation  \n**Zhiyong Wu** , Lingpeng Kong, Wei Bi, Xiang Li, Ben Kao.  \nACL 2021, long paper. [[pdf](https://arxiv.org/pdf/2105.14462.pdf)]\n[[code](https://github.com/LividWo/Revisit-MMT)]\n\n  19. Cascaded Head-colliding Attention  \nLin Zheng, **Zhiyong Wu** , Lingpeng Kong.  \nACL 2021, long paper. [[pdf](https://arxiv.org/pdf/2105.14850.pdf)]\n[[code](https://zywu.github.io/pub/wsdm2020.pdf)]\n\n**2020 and before**\n\n  20. Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT  \n**Zhiyong Wu** , Yun Chen, Ben Kao, Qun Liu.  \nACL 2020. [[pdf](https://arxiv.org/abs/2004.14786)]\n[[code](https://github.com/LividWo/Perturbed-Masking)]  \n\n  21. PERQ: Predicting, Explaining, and Rectifying Failed Questions in KB-QA Systems   \n**Zhiyong Wu** , Ben Kao, Tien-Hsuan Wu, Pengcheng Yin, Qun Liu.  \nWSDM 2020, long paper. [[pdf](https://zywu.github.io/pub/wsdm2020.pdf)]  \n\n  22. Towards Practical Open Knowledge Base Canonicalization   \nTTien-Hsuan Wu, **Zhiyong Wu** , Ben Kao, Pengcheng Yin.  \nCIKM 2018.\n[[pdf](https://www.cs.hku.hk/data/techreps/document/TR-2018-04.pdf)]  \n\n## Interns\n\n[Jiacheng Ye](https://jiacheng-ye.github.io/) |\n[EMNLP\'22a](https://arxiv.org/pdf/2209.14569.pdf),\n[EMNLP\'22b](https://arxiv.org/abs/2210.12329),\n[ICML\'23](https://arxiv.org/abs/2302.05698)  \n---|---  \n[Sijie Cheng](https://adacheng.github.io/) |\n[AAAI\'23](https://arxiv.org/abs/2211.11160)  \n[ Yaoxiang Wang](https://scholar.google.com/citations?user=7e_BZuYAAAAJ&hl=zh-\nCN) | [ACL\'23a](https://arxiv.org/abs/2212.10375),\n[ACL\'23b](https://arxiv.org/abs/2303.02913)  \n[Zhenyu Wu](https://github.com/numbmelon) |\n[ACL\'23b](https://arxiv.org/abs/2303.02913)  \n[Siyu Ren](https://drsy.github.io/) | [Under review at\nICLR\'24](https://arxiv.org/abs/2310.04691)  \n[Qiushi Sun](https://qiushisun.github.io/) | [Under review at\nICLR\'24](https://arxiv.org/abs/2310.00280)  \n[Fangzhi Xu](https://xufangzhi.github.io/) | TBA  \n[Kanzhi Cheng](https://scholar.google.com/citations?user=S2IPVnwAAAAJ&hl=zh-\nCN) | TBA  \n[Yi Lu](https://scholar.google.com/citations?user=WK62eYQAAAAJ&hl=zh-CN) | TBA\n\n'}
'''

# 找出所有单引号的位置
quote_positions = [pos for pos, char in enumerate(data) if char == "'"]

# 检查是否至少有两个单引号
if len(quote_positions) >= 4:
    # 获取第三个单引号的位置
    start_pos = quote_positions[2]
    # 获取最后一个单引号的位置
    end_pos = quote_positions[-1]
    # 提取所需文本
    extracted_text = data[start_pos+1:end_pos]
else:
    extracted_text = "不足以提取文本"

with open("target.md","w",encoding="utf-8") as f2:
    f2.write(extracted_text)

